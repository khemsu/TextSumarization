{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOKRGLow2BEXI0s+ZL8r9rX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khemsu/TextSumarization/blob/main/GradientBoostcodeSingleCell.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy pandas nltk scikit-learn rouge-score transformers\n",
        "!python -m nltk.downloader punkt stopwords averaged_perceptron_tagger_eng"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQzvv4Pwl8He",
        "outputId": "52f1bc24-59e5-4709-d449-0448af89d7d5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n",
            "Building wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=450cff7984df39c1a473b11445e1feec5c7c0b9ea28c07572fd00914b0da0475\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.1.2\n",
            "<frozen runpy>:128: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aOgwcatYfD32"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import pos_tag\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from rouge_score import rouge_scorer\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "articles_path = \"/Users/sundippahimlimbu/Gradient_Model/News Articles/business\"\n",
        "summaries_path = \"/Users/sundippahimlimbu/Gradient_Model/Summaries/business\"\n",
        "\n",
        "def read_text_files(filepath_list, folder_path):\n",
        "    \"\"\"Read all .txt files with proper encoding handling\"\"\"\n",
        "    contents = []\n",
        "    for filename in filepath_list:\n",
        "        with open(os.path.join(folder_path, filename), 'r', encoding='latin1') as f:\n",
        "            contents.append(f.read())\n",
        "    return contents\n",
        "\n",
        "article_files = sorted([f for f in os.listdir(articles_path) if f.endswith('.txt')])\n",
        "summary_files = sorted([f for f in os.listdir(summaries_path) if f.endswith('.txt')])\n",
        "\n",
        "article_texts = read_text_files(article_files, articles_path)\n",
        "summary_texts = read_text_files(summary_files, summaries_path)\n",
        "\n",
        "print(f\"Found {len(article_files)} articles and {len(summary_files)} summaries\")\n",
        "print(\"First 5 article filenames:\", article_files[:5])\n",
        "print(\"First 5 summary filenames:\", summary_files[:5])\n",
        "\n",
        "df = pd.DataFrame({'article': article_texts, 'summary': summary_texts})\n",
        "\n",
        "def preprocess(text):\n",
        "    \"\"\"Clean and tokenize text while preserving sentence structure\"\"\"\n",
        "    sentences = sent_tokenize(text)\n",
        "    clean_sentences = []\n",
        "    for sent in sentences:\n",
        "        # sentence to word\n",
        "        words = word_tokenize(sent.lower())\n",
        "\n",
        "        #words to alphnumeric only ie removing punctuations\n",
        "        words = [w for w in words if w.isalnum() and w not in stopwords.words('english')]\n",
        "\n",
        "        #joining into clean_sentences\n",
        "        clean_sentences.append(' '.join(words))\n",
        "    return clean_sentences\n",
        "\n",
        "preprocessed_articles = [preprocess(text) for text in article_texts]\n",
        "preprocessed_summaries = [preprocess(text) for text in summary_texts]\n",
        "\n",
        "def label_sentences(articles, summaries):\n",
        "    \"\"\"Generate binary labels (1=in summary, 0=not in summary)\"\"\"\n",
        "    y_labels = []\n",
        "    for art_sents, sum_sents in zip(articles, summaries):\n",
        "        labels = []\n",
        "        for art_sent in art_sents:\n",
        "            # Check if any summary sentence is contained in article sentence\n",
        "            match = 0\n",
        "            for sum_sent in sum_sents:\n",
        "                if sum_sent in art_sent or art_sent in sum_sent:\n",
        "                    match = 1\n",
        "                    break\n",
        "            labels.append(match)\n",
        "        y_labels.append(labels)\n",
        "    return y_labels\n",
        "\n",
        "def postprocess_labels(article, labels, min_sentence_length=5):\n",
        "    \"\"\"Ensure very short sentences or boilerplate are not selected.\"\"\"\n",
        "    for i, (sent, label) in enumerate(zip(article, labels)):\n",
        "        if len(sent.split()) < min_sentence_length:\n",
        "            labels[i] = 0  # Discard short sentences\n",
        "    return labels\n",
        "\n",
        "def extract_advanced_features(sentences):\n",
        "    \"\"\"Extract multiple linguistic features for each sentence\"\"\"\n",
        "    features = []\n",
        "    positions = np.linspace(0, 1, num=len(sentences))\n",
        "    tfidf = TfidfVectorizer().fit_transform(sentences)\n",
        "    tfidf_avg = np.array(tfidf.mean(axis=1)).flatten()\n",
        "\n",
        "    for i, sent in enumerate(sentences):\n",
        "        # Quantitative features\n",
        "        numbers = len(re.findall(r'\\$?\\d+(?:\\.\\d+)?%?', sent))\n",
        "        proper_nouns = len([word for word, tag in pos_tag(word_tokenize(sent)) if tag == 'NNP'])\n",
        "\n",
        "        # Structural features\n",
        "        word_count = len(word_tokenize(sent))\n",
        "        char_count = len(sent)\n",
        "        is_question = 1 if sent.strip().endswith('?') else 0\n",
        "\n",
        "        # Semantic features (simplified)\n",
        "        has_connector = 1 if any(word in sent for word in ['however', 'therefore', 'although']) else 0\n",
        "\n",
        "        features.append([\n",
        "            positions[i],      # Normalized position (0=start, 1=end)\n",
        "            tfidf_avg[i],     # TF-IDF importance\n",
        "            numbers,          # Count of numerical values\n",
        "            proper_nouns,     # Count of proper nouns\n",
        "            word_count,       # Word length\n",
        "            char_count/100,   # Normalized character length\n",
        "            is_question,      # Is question sentence\n",
        "            has_connector     # Contains discourse connector\n",
        "        ])\n",
        "\n",
        "    return np.array(features)\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Download all required NLTK data\n",
        "nltk.download([\n",
        "    'punkt',          # Tokenizer\n",
        "    'stopwords',      # Stopwords list\n",
        "    'wordnet',        # WordNet lemmatizer\n",
        "    'averaged_perceptron_tagger_eng'  # POS tagger\n",
        "])\n",
        "\n",
        "\n",
        "# Preprocess and label\n",
        "df['clean_article'] = df['article'].apply(preprocess)\n",
        "df['clean_summary'] = df['summary'].apply(preprocess)\n",
        "y_labels = label_sentences(df['clean_article'], df['clean_summary'])\n",
        "\n",
        "## 4. Model Training =====================================================\n",
        "\n",
        "# Feature extraction\n",
        "X_features = []\n",
        "for sentences in df['clean_article']:\n",
        "    features = extract_advanced_features(sentences)\n",
        "    X_features.append(features)\n",
        "\n",
        "# Prepare training data\n",
        "X = np.vstack(X_features)\n",
        "y = np.concatenate(y_labels)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Train calibrated model\n",
        "model = GradientBoostingClassifier(\n",
        "    n_estimators=200,\n",
        "    learning_rate=0.01,\n",
        "    max_depth=3,\n",
        "    random_state=42\n",
        ")\n",
        "calibrated_model = CalibratedClassifierCV(model, cv=5)\n",
        "calibrated_model.fit(X_train, y_train)\n",
        "\n",
        "print(f\"\\nTraining Accuracy: {calibrated_model.score(X_train, y_train):.2f}\")\n",
        "print(f\"Test Accuracy: {calibrated_model.score(X_test, y_test):.2f}\")\n",
        "\n",
        "def generate_summary(article, model, top_n=3):\n",
        "    \"\"\"Generate extractive summary with diversity and coherence\"\"\"\n",
        "    sentences = sent_tokenize(article)\n",
        "    if len(sentences) <= top_n:\n",
        "        return article\n",
        "\n",
        "    clean_sents = preprocess(article)\n",
        "    features = extract_advanced_features(clean_sents)\n",
        "    probas = model.predict_proba(features)[:, 1]\n",
        "\n",
        "    # Select top sentences with diversity\n",
        "    selected_indices = []\n",
        "    for _ in range(top_n):\n",
        "        remaining = [i for i in range(len(sentences)) if i not in selected_indices]\n",
        "        next_idx = remaining[np.argmax(probas[remaining])]\n",
        "        selected_indices.append(next_idx)\n",
        "\n",
        "    # Maintain original order\n",
        "    selected_indices.sort()\n",
        "    summary = ' '.join([sentences[i] for i in selected_indices])\n",
        "\n",
        "    # Post-processing\n",
        "    summary = summary[0].upper() + summary[1:]\n",
        "    if not summary.endswith(('.', '!', '?')):\n",
        "        summary = summary + '.'\n",
        "    return summary\n",
        "\n",
        "# Initialize ROUGE scorer\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "# Evaluate on sample\n",
        "sample_idx = 0\n",
        "generated = generate_summary(df['article'][sample_idx], calibrated_model)\n",
        "reference = df['summary'][sample_idx]\n",
        "\n",
        "print(\"\\n=== Original Article ===\")\n",
        "print(df['article'][sample_idx][:500] + \"...\")\n",
        "\n",
        "print(\"\\n=== Reference Summary ===\")\n",
        "print(reference)\n",
        "\n",
        "print(\"\\n=== Generated Summary ===\")\n",
        "print(generated)\n",
        "\n",
        "# ROUGE Evaluation\n",
        "scores = scorer.score(reference, generated)\n",
        "print(\"\\n=== ROUGE Scores ===\")\n",
        "print(f\"ROUGE-1: F1={scores['rouge1'].fmeasure:.3f} (Recall={scores['rouge1'].recall:.3f}, Precision={scores['rouge1'].precision:.3f})\")\n",
        "print(f\"ROUGE-2: F1={scores['rouge2'].fmeasure:.3f} (Recall={scores['rouge2'].recall:.3f}, Precision={scores['rouge2'].precision:.3f})\")\n",
        "print(f\"ROUGE-L: F1={scores['rougeL'].fmeasure:.3f} (Recall={scores['rougeL'].recall:.3f}, Precision={scores['rougeL'].precision:.3f})\")\n",
        "\n",
        "def generate_summary(article, model, top_n=3):\n",
        "    \"\"\"Generate extractive summary with diversity and coherence\"\"\"\n",
        "    sentences = sent_tokenize(article)\n",
        "    if len(sentences) <= top_n:\n",
        "        return article\n",
        "\n",
        "    clean_sents = preprocess(article)\n",
        "    features = extract_advanced_features(clean_sents)\n",
        "    probas = model.predict_proba(features)[:, 1]\n",
        "\n",
        "    # Select top sentences with diversity\n",
        "    selected_indices = []\n",
        "    for _ in range(top_n):\n",
        "        remaining = [i for i in range(len(sentences)) if i not in selected_indices]\n",
        "        next_idx = remaining[np.argmax(probas[remaining])]\n",
        "        selected_indices.append(next_idx)\n",
        "\n",
        "    # Maintain original order\n",
        "    selected_indices.sort()\n",
        "    summary = ' '.join([sentences[i] for i in selected_indices])\n",
        "\n",
        "    # Post-processing\n",
        "    summary = summary[0].upper() + summary[1:]\n",
        "    if not summary.endswith(('.', '!', '?')):\n",
        "        summary = summary + '.'\n",
        "    return summary\n",
        "\n",
        "# Initialize ROUGE scorer\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "# Evaluate on sample\n",
        "sample_idx = 0\n",
        "generated = generate_summary(df['article'][sample_idx], calibrated_model)\n",
        "reference = df['summary'][sample_idx]\n",
        "\n",
        "print(\"\\n=== Original Article ===\")\n",
        "print(df['article'][sample_idx][:500] + \"...\")\n",
        "\n",
        "print(\"\\n=== Reference Summary ===\")\n",
        "print(reference)\n",
        "\n",
        "print(\"\\n=== Generated Summary ===\")\n",
        "print(generated)\n",
        "\n",
        "# ROUGE Evaluation\n",
        "scores = scorer.score(reference, generated)\n",
        "print(\"\\n=== ROUGE Scores ===\")\n",
        "print(f\"ROUGE-1: F1={scores['rouge1'].fmeasure:.3f} (Recall={scores['rouge1'].recall:.3f}, Precision={scores['rouge1'].precision:.3f})\")\n",
        "print(f\"ROUGE-2: F1={scores['rouge2'].fmeasure:.3f} (Recall={scores['rouge2'].recall:.3f}, Precision={scores['rouge2'].precision:.3f})\")\n",
        "print(f\"ROUGE-L: F1={scores['rougeL'].fmeasure:.3f} (Recall={scores['rougeL'].recall:.3f}, Precision={scores['rougeL'].precision:.3f})\")\n"
      ]
    }
  ]
}